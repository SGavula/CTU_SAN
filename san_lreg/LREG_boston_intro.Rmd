---
title: "SAN linear regression -- Boston data"
output: html_document
date: "2023-10-02"
editor_options: 
  markdown: 
    wrap: 72
---

```{r libs, include=FALSE}
library("MASS") # collection of data sets and functions
library("ISLR") # data sets associated with the text
library("glmnet") # lasso
```

# Learn a simple regression model

This script stems from the lab accompanying Chapter 3 of Introduction to
Statistical Learning. We will use Boston data set available in MASS to
predict median house value in neighborhoods around Boston. In the
beginning we will work with the single independent variable: the
variable $lstat$ captures the percentage of population with lower
economical status in the given neighborhood (a town, an area). We will
learn the model and see its basic statistics.

```{r simple regression}
help(Boston) # learn the structure of the dataset, 506 areas described by 14 variables, medv will serve as the target variable

head(Boston)
View(Boston)
## Simple linear regression
## Start with a basic regression using the lm(y ~ x, data) function
lm.fit <- lm(medv ~ lstat, data = Boston) # lstat gives the percent of households with low socioeconomic status in the neighborhood
## Understand the model
summary(lm.fit) # see the model
names(lm.fit) # get all component names of the lm model
coef(lm.fit) # see the coefficeints once more, same as lm.fit$coefficients
confint(lm.fit) # confidence intervals around these estimates
sum(residuals(lm.fit)^2) # RSS
sqrt(sum(residuals(lm.fit)^2)/(nrow(Boston)-2)) # RSE, it can be interpreted as the estimate of standard deviation of the noise in our model
```

There are further ways to understand the model.

```{r regression plots}
plot(Boston$lstat, Boston$medv) # visualize the relationship between indep and dep variable
abline(lm.fit, col="red", lw=2) # visualize the fit

par(mfrow=c(2,2))
plot(lm.fit) # see the diagnostic plots

predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "confidence") # predict for three new neighborhoods including confints

plot(hatvalues(lm.fit)) # Leverage statistics can be computed for our predictors with hatvalues()
which.max(hatvalues(lm.fit)) # find the observation with the largest leverage
```

In summary, we can conclude the following. There is an obvious
relationship between the independent variable (household status) and the
dependent variable (median house value). Both the F-stat for the whole
model and t-stat for the predictor are significant, the predictor helps
to explain more than 50% of the variance in the target variable. The
median house price is around \$34500, on average it dercreases by \$950
with one percent increase of households with low socio-economical
status. For more than 30% it virtually becomes \$0 (the model does not
extrapolate well for these unobserved values). Both simple plot and
residual plot suggest that the relationship is rather non-linear.
Consequences: prediction could be improved with additional non-linear
terms, assumptions are violated, estimations do not have to be perfect
(namely p-values, confidence intervals).

**Further questions and tasks:**

1.  Construct a multiple linear regression model. Start with a full
    model that deals with all the independent variables. Compare it with
    the simple model above. Show at least two meaningful ways to compare
    models of different sizes. Did you improve the simple model?

```{r linear regression with all independent variables}
lm.full <- lm(medv ~ ., data = Boston)
summary(lm.full)
coef(lm.full) # see the coefficeints once more, same as lm.fit$coefficients
confint(lm.full) # confidence intervals around these estimates
sum(residuals(lm.full)^2) # RSS
```

### My answer

From the comparison of the simple and full linear regression models, the
full model improved the fit compared to the simple one:

-   **Residual Standard Error** decreased from 6.2 to 4.7, meaning the
    predictions are closer to the observed house prices.
-   **Multiple** $R^2$ increased from 0.544 to 0.741.
-   **Adjusted** $R^2$ rose from 0.543 to 0.734, showing that even after
    penalizing for additional variables, the model explains about 20%
    more variance.
-   **The Residual Sum of Squares** decrease significantly from 19472.38
    to 11078.78.

Then F-test was highly significant ($F = 108.1$,
$p < 2.2 \times 10^{-16}$), confirming a strong relationship between
variables and the outcome. Individual t-tests revealed that some
variables (such as indus and age) are not significant, suggesting
potential for simplifying the model.

Overall, the full model substantially improved performance by reducing
error and increasing explanatory power, while highlighting which
variables contribute most meaningfully.

2.  Propose a linear regression model of the right size. Focus on simple
    approaches, more advanced methods (shrinkage, etc.) will be
    practiced during the next lab.

```{r linear regression with all independent variables except indus and age}
lm.reduced_1 <- lm(medv ~ . - indus - age, data = Boston)
summary(lm.reduced_1)
coef(lm.reduced_1) # see the coefficeints once more, same as lm.fit$coefficients
confint(lm.reduced_1) # confidence intervals around these estimates
sum(residuals(lm.reduced_1)^2) # RSS
```

### My answer

From the output of linear regression with all variables, the variables
**indus** and **age** had the largest p-values, meaning they were not
statistically significant. Because of that I excluded them and refitted
the model. The results show that the model performance only slightly changed
compared to the full model:

-   **Residual Standard Error** stayed at about 4.7.
-   **Multiple** $R^2$ remained around 0.74.
-   **Adjusted** $R^2$ also stayed around 0.73.
-   **The Residual Sum of Squares** increased slightly from 11078.78 to
    11081.36, which is not a significant change.

Overall, the model was simplified without significant loss in performance.

```{r linear regression with all independent variables except cim, chas, indus and age}
lm.reduced_2 <- lm(medv ~ . - crim - chas - indus - age, data = Boston)
summary(lm.reduced_2)
coef(lm.reduced_2) # see the coefficeints once more, same as lm.fit$coefficients
confint(lm.reduced_2) # confidence intervals around these estimates
sum(residuals(lm.reduced_2)^2) # RSS
```

Then I tried to simplify model slightly more and I excluded **crim** and
**chas** from reduced model, which had relatively high p-values compared
to other variables. Output shows that the model performance became worse (higher
**Residual Standard Error** and **the Residual Sum of Squares**, lower
**Adjusted** $R^2$). Since all the remaining variables have smaller
p-values than **crim** and **chas**, it follows that they contribute
even more strongly to the model. Therefore, removing any of them would
likely reduce the explanatory power even further.
