---
title: "Moving beyond linearity -- fitting a sine function"
output:
  html_document:
    df_print: paged
---

Let us work with a noisy trigonometric **sine** function. It is clearly non-linear, and we will attempt to fit it using models of varying complexity. In the beginning, we will assess how well a linear model performs. Then, we will propose a more suitable alternative model. Before you run the script, take a moment to think about the best model yourself.

```{r prepare, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(splines)
library(gam)

# root mean squared error function for later evaluation
myRMSE = function(m, o){
  sqrt(mean((m - o)^2))
}
```

## Data generation

```{r sine generation}
x <- seq(0, pi * 2, 0.1)
#y <- sin(x)
y <- sin(x) + rnorm(n = length(x), mean = 0, sd = sd(sin(x) / 2))
sin.data <- data.frame(y,x)
# independent dataset for later testing
sin.test <- data.frame(x=seq(0, pi * 2, 0.1),y=sin(x) + rnorm(n = length(x), mean = 0, sd = sd(sin(x) / 2)))
plot(sin.data$x,sin.data$y)
lines(sin.data$x,sin(sin.data$x),col="green")
```

## How far a linear model can go?

<!-- When modeling a single period of the sine function, the linear model performs significantly better than the intercept-only model, but it still severely underfits the sine wave.-->

```{r sine with linear model}
lm.sin <- lm(y ~ x, data = sin.data) # learn the linear model
lm.sin.pred <- predict(lm.sin, interval="confidence") # make predictions at testing points
summary(lm.sin) # the model works somehow, explains more than 40% of the variance, much better than averaging
```
```{r linear model plots, echo=FALSE}
plot(sin.data$x,sin.data$y)
abline(lm.sin,col="red")

lines(sin.data$x,lm.sin.pred[,"lwr"],col="blue",lty=2)
lines(sin.data$x,lm.sin.pred[,"upr"],col="blue",lty=2)
lines(sin.data$x,sin(sin.data$x),col="green")
plot(lm.sin,which=1) # however, the assumptions violated, unreliable confidence intervals, the model underfits the data
```

The **linear model** underfits the sine wave. It is unable to capture the relationship between the dependent and independent variable, it has a high error rate both on training as well as testing data (`r round(myRMSE(predict(lm.sin,sin.data),sin.data$y),4)` and `r round(myRMSE(predict(lm.sin,sin.test),sin.test$y),4)`, respectively). For one sine period, the linear model overcomes the **zero model** (`r round(myRMSE(0,sin.data$y),4)` and `r round(myRMSE(0,sin.test$y),4)`, respectively), explains more than 40% of variance in the data, but we definitely need a more complex model. The cubic polynomial could be the first choice, its S shape with two inflection points resembles sine, although it will never fit the sine function perfectly for shape differences.

Before moving to nonlinear models, we demonstrate that lm() and gam() produce essentially the same results for models with closed-form solutions. Although the underlying algorithms differ (OLS vs. backfitting), any differences are negligible for simple linear tasks. The main difference for us would be that **generalized additive models (GAMs)** also allow for smooth terms, such as smoothing splines or local regression, which do not have closed-form solutions.

<!-- You may also note that GAMs are scored with deviance and AIC rather than with RSS, R2 and adjusted R2. We will explain these scores later when dealing with GLMs. -->

```{r gam vs lm}
gam.lin<-gam(y~x,data=sin.data) # simple gam, matches lm
summary(gam.lin)
coefficients(gam.lin) # the same as in lm.sin
```
```{r gam vs lm plots, echo=FALSE}
plot(sin.data$x,sin.data$y)
lines(sin.data$x,sin(sin.data$x),col="green") # plot the true model
legend("topright",legend=c("sine","linear GAM"),lty=c("solid"),col=c("green","red"),bty="n")
abline(gam.lin,col="red")
```

## Non-linear models

### Polynomial regression

We will begin with polynomial models, and then proceed to splines, exploring the optimal parametrization for each model type.

```{r sine with cubic model}
lm.sin.cub <- lm(y ~ poly(x,3), data = sin.data) # learn the cubic polynomial model
lm.sin.pred <- predict(lm.sin.cub, interval="confidence") # make predictions at testing points
summary(lm.sin.cub) # the model works even better, explains nearly 80% of the variance, much better than linear model
anova(lm.sin,lm(y ~ poly(x,2), data = sin.data),lm.sin.cub,lm(y ~ poly(x,4), data = sin.data),lm(y ~ poly(x,5), data = sin.data),lm(y ~ poly(x,6), data = sin.data),lm(y ~ poly(x,7), data = sin.data))
```

The **cubic polynomial** performs much better than the linear fit. It explains nearly 80% of the variance and its training and testing root mean square error decrease at `r round(myRMSE(predict(lm.sin.cub,sin.data),sin.data$y),4)` and ``r round(myRMSE(predict(lm.sin.cub,sin.test),sin.test$y),4)`, respectively. Increasing the polynomial degree shows that odd-degree polynomials fit the sine wave better, as sine itself is an odd function. ANOVA suggests that higher odd-degree polynomials (degree 5 and 7) can provide a slight improvement over the cubic polynomial.

```{r cubic model plots, echo=FALSE}
plot(sin.data$x,sin.data$y)

lines(sin.data$x,lm.sin.pred[,"fit"],col="blue")
lines(sin.data$x,lm.sin.pred[,"lwr"],col="blue",lty=2)
lines(sin.data$x,lm.sin.pred[,"upr"],col="blue",lty=2)
lines(sin.data$x,sin(sin.data$x),col="green")
legend("topright",legend=c("sine","cubic polynmial"),lty=c("solid"),col=c("green","blue"),bty="n")
plot(lm.sin.cub,which=1) # much better residuals
```

### Splines

In order to reach a better fit, we will demonstrate the application of splines. Let us start with the linear spline. The obvious choice is to work with two knots at $\pi/2$ and $3\pi/2$.

```{r linear spline}
gam.lins<-gam(y~bs(x,degree=1,knots=c(pi/2,3*pi/2)),data=sin.data) # gam with a linear spline with two knots
summary(gam.lins)
coefficients(gam.lins)
gam.lins.pred <- predict(gam.lins, se.fit=T) # make predictions at testing points
```

```{r linear spline plots, echo=FALSE}
plot(sin.data$x,sin.data$y) # plot the original data
lines(sin.data$x,sin(sin.data$x),col="green") # plot the true model
lines(sin.data$x,gam.lins.pred$fit,col="blue")
legend("topright",legend=c("sine","linear spline"),lty=c("solid"),col=c("green","blue"),bty="n")
```

**Linear splines** with a small number of knots provide a rough piecewise-linear approximation of a sine wave. They are simple and interpretable but have limited accuracy, especially for functions with strong curvature like sine. Note, that we used domain knowledge to set the knots. In practice, the true function is most often not known and the knots have to be set suboptimally.

The **quadratic spline** with one knot at $\pi$ is the simplest choice with the desired S shape with two inflection points, we will also play with the cubic spline and a large-degree spline. 

```{r quadratic and cubic splines}
gam.quad<-gam(y~bs(x,degree=2,knots=c(pi)),data=sin.data) # gam with a quadratic spline with one knot
gam.quad.pred <- predict(gam.quad, se.fit=T) # make predictions at testing points
summary(gam.quad)
coefficients(gam.quad)
gam.cub<-gam(y~bs(x,degree=3,knots=c(pi)),data=sin.data) # increase the degree, cubic spline with one knot
gam.overfit<-gam(y~bs(x,degree=16,knots=quantile(sin.data$x,c(0.25,0.5,0.75))),data=sin.data) # overfit the data
```

```{r quadratic and cubic spline plots, echo=FALSE}
plot(sin.data$x,sin.data$y) # plot the original data
lines(sin.data$x,sin(sin.data$x),col="green") # plot the true model
lines(sin.data$x,gam.quad.pred$fit,col="blue")
lines(sin.data$x,predict(gam.cub),col="red")
lines(sin.data$x,predict(gam.overfit),col="orange")
legend("topright",legend=c("sine","quadratic spline","cubic spline","degree-16 spline"),lty=c("solid"),col=c("green","blue","red","orange"),bty="n")
```

How well does it compare with smoothing splines?
```{r sine with smoothing spline}
gam.smooth<-gam(y~s(x),data=sin.data) # gam with a smoothing spline, the default flexibility is df=4 (df=1 corresponds to linear fit)
#gam.smooth<-gam(y~s(x,df=nrow(sin.data)),data=sin.data) # smoothing spline, exact fit
summary(gam.smooth)
coefficients(gam.smooth)
gam.smooth.pred <- predict(gam.smooth, se.fit=T) # make predictions at testing points
# the plot below suggests that the smoothing spline with df=4 has about the right flexibility, however learn the best df value with CV
gam.smooth.opt<-smooth.spline(sin.data$x, sin.data$y, cv = TRUE)
gam.smooth.opt # the best df seems to be a bit larger, around 6
```

```{r smoothing spline plots, echo=FALSE}
plot(sin.data$x,sin.data$y) # plot the smoothing spline
lines(sin.data$x,sin(sin.data$x),col="green") # plot the true model
lines(sin.data$x,gam.smooth.pred$fit,col="blue")
lines(sin.data$x,gam.smooth.pred$fit+2*gam.smooth.pred$se.fit[,1],col="blue",lty=2)
lines(sin.data$x,gam.smooth.pred$fit-2*gam.smooth.pred$se.fit[,1],col="blue",lty=2)
legend("topright",legend=c("sine","degree-4 smoothing"),lty=c("solid"),col=c("green","blue"),bty="n")
plot(sin.data$x,sin.data$y) # plot the smoothing spline
lines(sin.data$x,sin(sin.data$x),col="green") # plot the true model
lines(sin.data$x,gam.smooth.pred$fit,col="blue")
lines(sin.data$x,predict(gam.smooth.opt)$y,col="red")
legend("topright",legend=c("sine","degree-4 smoothing","degree-opt smoothing"),lty=c("solid"),col=c("green","blue","red"),bty="n")
```

### Step functions

```{r sine with step functions}
# the most simple option
lm.step <- lm(y ~ cut(sin.data$x,2), data = sin.data)
# the overfitted option
lm.step.overfit <- lm(y ~ cut(sin.data$x,trunc(nrow(sin.data)/2)), data = sin.data)
```

```{r step function plots, echo=FALSE}
# plot both the options
plot(sin.data$x,sin.data$y)
lines(sin.data$x,sin(sin.data$x),col="green")
lines(sin.data$x, predict(lm.step,sin.data), col = "blue")
lines(sin.data$x, predict(lm.step.overfit,sin.data), col = "red")
legend("topright",legend=c("sine","simple step function","overfitted step function"),lty=c("solid"),col=c("green","blue","red"),bty="n")
```

We have shown that a model with two **steps** underfits the data. A second option with approximately 30 steps does the opposite: each step’s estimate is based on only about two samples, which is too few. Ideally, each step should include 5–10 samples to reduce variance in the mean estimate, leading to models with roughly 6–12 steps. The exact optimal number can be determined using cross-validation.

## Summary

Let us compute the root mean square errors (RMSE) of all models in one place, starting with the **testing** (noisy) data. The first two lines report the performance of the baseline models: the least and the most informed. Note that there is an irreducible error in this data caused by noise as evident from the performance of the true sine model. The testing errors reported below should therefore be larger than this baseline. The **training errors** further illustrate the effect of overfitting: if they are substantially smaller than the testing errors, the model is overfitting the **training** data. Finally, we compute the RMSE of each model against the **ideal sine** function to assess how accurately they reconstruct the original signal.

```{r final evaluation, echo=FALSE}
# Define the models and predictions
models <- c("zero benchmark","true sine benchmark", "linear", "cubic polynomial", "20th degree polynomial", "two steps", "10 steps", "50 steps", "linear spline with two knots","quadratic spline with one knot", "cubic spline with one knot"," spline with 20 dfs", "smoothing spline with 4 dfs", paste("opt smoothing with", round(gam.smooth.opt$df,2), "dfs"))

# Compute RMSEs
rmse_test <- c(
  myRMSE(0, sin.test$y),
  myRMSE(sin(sin.test$x), sin.test$y),
  myRMSE(predict(lm.sin, sin.test), sin.test$y),
  myRMSE(predict(lm.sin.cub, sin.test), sin.test$y),
  myRMSE(predict(lm(y ~ poly(x,20), data = sin.data), sin.test), sin.test$y),
  myRMSE(predict(lm.step, sin.test), sin.test$y),
  myRMSE(predict(lm(y ~ cut(x,20), data = sin.data), sin.test), sin.test$y),
  myRMSE(predict(lm.step.overfit, sin.test), sin.test$y),
  myRMSE(predict(gam.lins, sin.test), sin.test$y),
  myRMSE(predict(gam.quad, sin.test), sin.test$y),
  myRMSE(predict(gam.cub, sin.test), sin.test$y),
  myRMSE(predict(gam.overfit, sin.test), sin.test$y),
  myRMSE(predict(gam.smooth, sin.test), sin.test$y),
  myRMSE(predict(gam.smooth.opt, sin.test$x)$y, sin.test$y)
)

rmse_train <- c(
  myRMSE(0, sin.data$y),
  myRMSE(sin(sin.data$x), sin.data$y),
  myRMSE(predict(lm.sin, sin.data), sin.data$y),
  myRMSE(predict(lm.sin.cub, sin.data), sin.data$y),
  myRMSE(predict(lm(y ~ poly(x,20), data = sin.data), sin.data), sin.data$y),
  myRMSE(predict(lm.step, sin.data), sin.data$y),
  myRMSE(predict(lm(y ~ cut(x,20), data = sin.data), sin.data), sin.data$y),
  myRMSE(predict(lm.step.overfit, sin.data), sin.data$y),
  myRMSE(predict(gam.lins, sin.data), sin.data$y),
  myRMSE(predict(gam.quad, sin.data), sin.data$y),
  myRMSE(predict(gam.cub, sin.data), sin.data$y),
  myRMSE(predict(gam.overfit, sin.data), sin.data$y),
  myRMSE(predict(gam.smooth, sin.data), sin.data$y),
  myRMSE(predict(gam.smooth.opt, sin.data$x)$y, sin.data$y)
)

rmse_ideal <- c(
  myRMSE(0, sin(sin.data$x)),
  myRMSE(sin(sin.data$x), sin(sin.data$x)),
  myRMSE(predict(gam.lin),sin(sin.data$x)),
  myRMSE(predict(lm.sin.cub),sin(sin.data$x)),
  myRMSE(predict(lm(y ~ poly(x,20), data = sin.data)), sin(sin.data$x)),
  myRMSE(predict(lm.step), sin(sin.data$x)),
  myRMSE(predict(lm(y ~ cut(x,20), data = sin.data)), sin(sin.data$x)),
  myRMSE(predict(lm.step.overfit), sin(sin.data$x)),
  myRMSE(predict(gam.lins),sin(sin.data$x)),
  myRMSE(predict(gam.quad),sin(sin.data$x)),
  myRMSE(predict(gam.cub),sin(sin.data$x)),
  myRMSE(predict(gam.overfit),sin(sin.data$x)),
  myRMSE(predict(gam.smooth),sin(sin.data$x)),
  myRMSE(predict(gam.smooth.opt)$y,sin(sin.data$x))
)

# Combine into a single data frame
rmse_table <- data.frame(
  Model = models,
  Testing = round(rmse_test,4),
  Training = round(rmse_train,4),
  Ideal = round(rmse_ideal,4),
  check.names = FALSE
)

# Print nicely
knitr::kable(rmse_table, caption = "RMSE of models for testing, training, and ideal sine data")
```
The quadratic spline with a single knot performed well in our scenario. This spline provided sufficient flexibility to fit the sine function without overfitting. The disadvantage is that we had to select the knot appropriately. The other simple non-linear functions, such as the degree-3 polynomial, cubic spline, and smoothing spline, also performed adequately. More complex models, however, tend to overfit the data. On the other hand, the degrees, steps or knot numbers have to be set high to see the overfitting effect at large. The amount of noise must also be considered when comparing models. Our default setting contains a relatively large amount of noise, which increases the risk of overfitting. If learning from less noisy data, more complex models could become advantageous.


